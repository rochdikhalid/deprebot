{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style = \"text-align: center; color: #6E3A91;\"> ASTROBOT </h1>\n",
    "<h2 style = \"text-align: center; font-weight: lighter;\"> Boost your Mental Health </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dependencies ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, random, re, string, pickle\n",
    "import numpy as np\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Phase I - Data Cleaning ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the data\n",
    "with open('../data/root.json') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Phase II - Preparing data for training ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Natural Language Processing (NLP) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instances\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_words = []\n",
    "categories = []\n",
    "directories = []\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        # Tokenization\n",
    "        tokens = tokenizer.tokenize(pattern)   \n",
    "        re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "        tokens = [re_punc.sub('', w) for w in tokens]\n",
    "        tokens = [word for word in tokens if word.isalpha()]\n",
    "        # Lemmatization\n",
    "        tokens = [lemmatizer.lemmatize(w.lower()) for w in tokens]\n",
    "        # Remove stp words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [w for w in tokens if not w in stop_words]\n",
    "        \n",
    "        tokenized_words.extend(tokens)\n",
    "        directories.append((tokens, intent['tag']))\n",
    "        \n",
    "        if intent['tag'] not in categories:\n",
    "            categories.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some sorting\n",
    "tokenized_words = sorted(list(set(tokenized_words)))\n",
    "categories = sorted(list(set(categories)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143 directories\n",
      "53 categories ['bot_profile_age', 'bot_profile_info', 'bot_profile_interest', 'bot_profile_location', 'bot_profile_name', 'goodbye', 'greetings', 'group_1', 'group_10', 'group_11', 'group_12', 'group_13', 'group_14', 'group_15', 'group_16', 'group_17', 'group_18', 'group_19', 'group_2', 'group_20', 'group_21', 'group_22', 'group_23', 'group_24', 'group_25', 'group_26', 'group_27', 'group_28', 'group_29', 'group_3', 'group_30', 'group_31', 'group_32', 'group_33', 'group_34', 'group_35', 'group_36', 'group_37', 'group_38', 'group_39', 'group_4', 'group_40', 'group_41', 'group_42', 'group_43', 'group_44', 'group_45', 'group_5', 'group_6', 'group_7', 'group_8', 'group_9', 'thanks']\n",
      "162 tokenized words ['abandonment', 'affect', 'affected', 'afraid', 'age', 'ago', 'along', 'always', 'anxiety', 'anxious', 'anymore', 'anyone', 'anything', 'ask', 'away', 'awesome', 'back', 'bad', 'better', 'biological', 'break', 'brother', 'bullying', 'bye', 'ca', 'call', 'care', 'case', 'cause', 'chatting', 'chemistry', 'come', 'common', 'control', 'could', 'country', 'cry', 'daily', 'day', 'depressed', 'depression', 'diabetes', 'disappointing', 'disappointment', 'doe', 'effect', 'end', 'everyone', 'experienced', 'fail', 'failure', 'family', 'far', 'favorite', 'fear', 'feel', 'feeling', 'file', 'find', 'first', 'forever', 'gifted', 'going', 'good', 'goodbye', 'greeting', 'group', 'ha', 'hate', 'hello', 'helpful', 'helping', 'hey', 'hi', 'highest', 'interest', 'intersted', 'kind', 'kinda', 'lack', 'last', 'lately', 'later', 'lead', 'leading', 'leave', 'let', 'letting', 'life', 'like', 'location', 'lonely', 'long', 'losing', 'love', 'lowest', 'make', 'many', 'may', 'mean', 'meaning', 'meet', 'mind', 'much', 'name', 'named', 'never', 'next', 'nice', 'normal', 'nothing', 'nt', 'old', 'one', 'oversleeping', 'parent', 'peak', 'prone', 'quit', 'race', 'rate', 'really', 'reduce', 'rule', 'run', 'sad', 'scary', 'school', 'see', 'side', 'sister', 'sleep', 'soon', 'sorry', 'stage', 'stay', 'stress', 'stressed', 'succeed', 'suffer', 'survive', 'take', 'temporary', 'thank', 'thanks', 'think', 'thought', 'till', 'time', 'top', 'touch', 'trying', 'ugly', 'useless', 'victim', 'wa', 'want', 'wassup', 'winner', 'wish', 'world', 'ya']\n"
     ]
    }
   ],
   "source": [
    "print (len(directories), \"directories\")\n",
    "print (len(categories), \"categories\", categories)\n",
    "print (len(tokenized_words), \"tokenized words\", tokenized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create the traing dataset #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding (bag of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "# Create an empty list for the output\n",
    "output_empty = [0] * len(categories)\n",
    "# Training set, bag of words for each sentence\n",
    "for directory in directories:\n",
    "    # Initialize a bag of words\n",
    "    bag = []\n",
    "    # List of tokenized words for the pattern\n",
    "    pattern_words = directory[0]\n",
    "    # Create the bag of words array with 1, if word match found in current pattern\n",
    "    for word in tokenized_words:\n",
    "        if word in pattern_words:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "            \n",
    "    # Output is a '0' for each tag and '1' for current tag (for each pattern)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[categories.index(directory[1])] = 1\n",
    "    \n",
    "    dataset.append([bag, output_row])\n",
    "\n",
    "# To randoize the output\n",
    "random.shuffle(dataset)\n",
    "dataset = np.array(dataset, dtype = 'object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X 'pattern: 0':  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Y 'intent: 0':  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "x = list(dataset[:, 0])\n",
    "y = list(dataset[:, 1])\n",
    "print(\"X 'pattern: 0': \", x[0])\n",
    "print(\"Y 'intent: 0': \", y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113 113 30 30\n",
      "162 53 162 53\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = x[30:], y[30:], x[:30], y[:30]\n",
    "# To make sure the dataset is splitted correctly\n",
    "print(len(x_train), len(y_train), len(x_test), len(y_test))\n",
    "print(len(x_train[0]), len(y_train[0]), len(x_test[0]), len(y_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Phase III - Training ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Phase IV - Testing ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Phase V - Saving the Final Model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
